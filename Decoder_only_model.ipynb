{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeLgxwY50OjwZiHaI9mDfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaeSimcheon/torch/blob/main/Decoder_only_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "07/08 24분 39초"
      ],
      "metadata": {
        "id": "9cyJz2rhJrm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "class config :\n",
        "  hidden_size = 512\n",
        "  num_layers = 4\n",
        "  num_heads = 8\n",
        "  voca_size = 10000\n",
        "  ffn_size = 4*hidden_size\n",
        "  block_size = 16\n",
        "  batch_size = 8\n",
        "  dropout_rate = 0.1\n",
        "  device = 'cpu'"
      ],
      "metadata": {
        "id": "6oMlL0HiD_WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.affine_q = torch.nn.Linear(config.hidden_size,config.hidden_size)\n",
        "    self.affine_k = torch.nn.Linear(config.hidden_size,config.hidden_size)\n",
        "    self.affine_v = torch.nn.Linear(config.hidden_size,config.hidden_size)\n",
        "    self.dropout_alpha = torch.nn.Dropout(config.dropout_rate)\n",
        "    self.affine_o = torch.nn.Linear(config.hidden_size,config.hidden_size)\n",
        "\n",
        "  def forward(self,query,key,value):\n",
        "    B,L =query.shape[0],query.shape[1]\n",
        "    S = key.shape[1]\n",
        "    H = self.config.hidden_size\n",
        "    NH = self.config.num_heads\n",
        "    HH = H // NH\n",
        "\n",
        "    q = self.affine_q(query).view(B,L,NH,HH).transpose(2,1)\n",
        "    k = self.affine_k(key).view(B,S,NH,HH).transpose(2,1)\n",
        "    v = self.affine_v(value).view(B,S,NH,HH).transpose(2,1)\n",
        "\n",
        "    qk = torch.matmul(q,k.transpose(3,2)) / HH\n",
        "    mask = torch.triu(torch.full((L,S),float('-inf'),device= qk.device),diagonal=1)\n",
        "    qk += mask.view(1,1,L,S)\n",
        "\n",
        "    alpha = torch.softmax(qk,dim = -1)\n",
        "    alpha = self.dropout_alpha(alpha)\n",
        "\n",
        "    scores = torch.matmul(alpha,v).transpose(2,1).reshape(B,L,H)\n",
        "    output = self.affine_o(scores)\n",
        "    return output\n",
        "\n",
        "class Layer(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.norm_mha = torch.nn.LayerNorm(config.hidden_size)\n",
        "    self.mha = MultiHeadAttention(config)\n",
        "    self.dropout_mha = torch.nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    self.norm_ffn = torch.nn.LayerNorm(config.hidden_size)\n",
        "    self.ffn = torch.nn.Linear(config.hidden_size,config.ffn_size)\n",
        "    self.gelu = torch.nn.functional.gelu\n",
        "    self.dropout_ffn = torch.nn.Dropout(config.dropout_rate)\n",
        "    self.rev_ffn = torch.nn.Linear(config.ffn_size,config.hidden_size)\n",
        "\n",
        "  def _mha(self,inputs):\n",
        "    return self.dropout_mha(self.mha(inputs,inputs,inputs))\n",
        "\n",
        "  def _ffn(self,inputs):\n",
        "    inter = self.gelu(self.ffn(inputs))\n",
        "    return self.rev_ffn(self.dropout_ffn(inter))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    x = inputs\n",
        "\n",
        "    x = x + self._mha(self.norm_mha(x))\n",
        "    x = x + self._ffn(self.norm_ffn(x))\n",
        "    return x\n",
        "\n",
        "class Block(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layers = torch.nn.ModuleList([Layer(config) for _ in range(config.num_layers)])\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    x = inputs\n",
        "\n",
        "    for layer in self.layers :\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.pos_embedding = torch.nn.Embedding(config.block_size,config.hidden_size)\n",
        "    self.voc_embedding = torch.nn.Embedding(config.voca_size,config.hidden_size)\n",
        "    self.block = Block(config)\n",
        "    self.output = torch.nn.Linear(config.hidden_size,config.voca_size)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "\n",
        "    position = torch.arange(0,self.config.block_size,dtype= torch.int,device = inputs.device)\n",
        "    pos=self.pos_embedding(position)\n",
        "    emb=self.voc_embedding(inputs)\n",
        "\n",
        "    emb += pos.view(1,pos.shape[0],pos.shape[1])\n",
        "\n",
        "    embedded=self.block(emb)\n",
        "    output = self.output(embedded)\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "P6oQwF5LER_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query = torch.randn(config.batch_size,config.block_size,config.hidden_size)\n",
        "inputs = torch.randint(0,config.voca_size,(config.batch_size,config.block_size),dtype = torch.int,device = config.device)\n",
        "model = Model(config)\n",
        "model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra5dCMsJFvf2",
        "outputId": "74d95db3-8409-4420-9c59-7b8091038f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.7517e-01, -3.4024e-01,  2.4313e-01,  ...,  3.4671e-01,\n",
              "           3.7785e-01,  6.8465e-01],\n",
              "         [-8.7872e-01,  2.3702e-02,  6.1253e-02,  ...,  6.9860e-01,\n",
              "          -2.2357e-01, -9.2825e-01],\n",
              "         [ 2.0150e-01,  8.4275e-01, -1.5915e+00,  ..., -1.1473e+00,\n",
              "          -1.4977e+00, -1.6272e-01],\n",
              "         ...,\n",
              "         [ 9.5123e-01, -4.0271e-01,  9.4105e-01,  ..., -2.2509e-01,\n",
              "           1.8943e-01,  5.7453e-01],\n",
              "         [-1.1927e+00,  1.2911e+00, -1.4250e+00,  ...,  9.0217e-02,\n",
              "           4.5180e-01, -1.2284e+00],\n",
              "         [-1.6026e+00,  1.3862e+00, -7.5816e-01,  ..., -9.9520e-01,\n",
              "           4.5855e-01,  1.1881e-01]],\n",
              "\n",
              "        [[-7.0109e-01,  5.0281e-01,  5.2916e-02,  ...,  2.1408e+00,\n",
              "          -7.0497e-01, -6.2794e-03],\n",
              "         [-3.2927e-01,  1.2641e-01,  4.9392e-01,  ...,  1.0467e+00,\n",
              "           8.7958e-02,  5.2763e-01],\n",
              "         [ 3.8311e-01, -5.7107e-01, -1.0126e+00,  ..., -2.0388e-01,\n",
              "          -9.5306e-01, -3.2201e-01],\n",
              "         ...,\n",
              "         [ 1.0934e+00, -9.9820e-01,  1.7156e+00,  ..., -2.1213e-01,\n",
              "           3.3188e-04, -5.9687e-02],\n",
              "         [-4.4896e-01,  1.6192e-01,  3.5126e-01,  ..., -1.2474e+00,\n",
              "           4.0627e-01,  3.9526e-01],\n",
              "         [-1.2015e+00,  4.6625e-01, -1.7777e-01,  ..., -1.4722e-01,\n",
              "          -6.7505e-01, -4.0790e-02]],\n",
              "\n",
              "        [[-1.8930e-01,  2.2296e-01,  7.9154e-01,  ..., -6.6269e-01,\n",
              "           8.7493e-01,  1.8019e+00],\n",
              "         [-1.0333e+00, -1.2319e-01,  1.4380e+00,  ...,  8.9377e-01,\n",
              "          -3.2118e-01,  1.3645e-01],\n",
              "         [ 4.7686e-01, -3.3363e-01, -9.2763e-01,  ...,  2.0046e-01,\n",
              "           2.1974e-01,  1.0628e+00],\n",
              "         ...,\n",
              "         [-2.4167e-01, -8.7672e-01,  1.4580e+00,  ..., -2.7416e-01,\n",
              "           6.9484e-01,  3.4601e-01],\n",
              "         [-2.5575e-01,  9.4349e-01, -8.3061e-01,  ..., -1.0210e+00,\n",
              "           1.0085e+00, -1.8424e-02],\n",
              "         [-1.2797e+00, -4.1295e-02, -1.0163e+00,  ..., -1.1784e+00,\n",
              "           7.7551e-01, -8.2543e-02]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 2.8579e-01, -2.1599e-01, -3.8446e-01,  ...,  2.3143e+00,\n",
              "          -2.2492e-01,  7.9122e-01],\n",
              "         [ 3.0228e-01,  2.4240e-01,  2.0101e+00,  ...,  1.4264e+00,\n",
              "          -5.5161e-02,  8.2023e-01],\n",
              "         [-1.7236e-01,  3.6065e-01, -7.7683e-01,  ..., -1.1842e+00,\n",
              "          -6.5902e-01,  3.8479e-01],\n",
              "         ...,\n",
              "         [ 6.3485e-01,  4.8094e-02, -3.8730e-01,  ...,  5.4551e-01,\n",
              "           7.2285e-01,  4.9609e-01],\n",
              "         [ 6.2373e-01, -2.9660e-02, -1.9285e-01,  ..., -9.7086e-01,\n",
              "           8.7409e-01,  3.9578e-01],\n",
              "         [-8.3611e-01,  1.1502e+00, -7.3943e-01,  ..., -3.6767e-01,\n",
              "          -1.2398e-01,  7.1926e-01]],\n",
              "\n",
              "        [[-6.6610e-01, -1.2407e+00, -1.7571e-01,  ...,  1.0584e-01,\n",
              "          -7.8622e-01,  1.3549e+00],\n",
              "         [-9.1441e-01,  4.4351e-01,  4.5442e-01,  ...,  6.4713e-01,\n",
              "          -5.6707e-01, -2.0726e-01],\n",
              "         [-2.3473e-01, -1.0403e+00, -9.9226e-01,  ..., -1.2219e+00,\n",
              "          -7.9195e-02,  4.1660e-03],\n",
              "         ...,\n",
              "         [ 1.0919e+00, -1.1160e+00,  5.1416e-01,  ...,  1.3607e-01,\n",
              "          -2.4618e-01,  8.7139e-01],\n",
              "         [-3.7335e-01,  1.1627e+00, -2.0318e-01,  ...,  1.8493e-01,\n",
              "           7.4344e-01,  8.6625e-01],\n",
              "         [-1.1577e+00,  2.2918e-01, -3.4309e-01,  ..., -1.5623e+00,\n",
              "          -7.5107e-01,  8.9703e-01]],\n",
              "\n",
              "        [[-5.3816e-01, -9.0279e-01, -1.4534e-01,  ...,  7.1607e-01,\n",
              "          -3.1865e-01,  1.1697e+00],\n",
              "         [ 6.2130e-02,  3.8783e-03,  1.0092e+00,  ..., -8.6931e-02,\n",
              "          -6.4529e-01,  1.0445e+00],\n",
              "         [ 3.5067e-01, -3.4447e-01, -6.8294e-01,  ..., -8.6953e-01,\n",
              "          -1.1806e-01, -5.1139e-01],\n",
              "         ...,\n",
              "         [ 1.3008e+00, -5.2414e-01,  3.2087e-01,  ...,  6.8761e-01,\n",
              "          -1.1731e+00,  6.0328e-01],\n",
              "         [ 1.2481e+00,  5.3197e-01, -2.0400e-01,  ..., -7.3629e-01,\n",
              "           1.2942e+00,  2.0895e-02],\n",
              "         [-1.8201e-02,  8.0112e-01, -8.2048e-01,  ..., -8.7048e-02,\n",
              "          -9.1023e-01,  9.6481e-02]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}