{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67D69KKWlZO5"
      },
      "source": [
        "0509 (1)\n",
        "\n",
        "- hugging face datasets library를 이용하여 robert 한국어 tokenizer를 이용하여 kosquad data를 다루어 보았다.\n",
        "\n",
        "- 원래 내가 직접 짠 코드로 구현해보고 싶었으나, 영문과 한글이 바이트로 나타내면 다르기 때문에 offset 계산 하는 것이 까다로웠고, pre_trained tokenizer를 사용하는 것을 선택했다.\n",
        "\n",
        "- 설명들 들어보니 sentencePiece 방법을 적용하는 토크나이징했다고 한다.\n",
        "\n",
        "- 간단하게 몇가지 옵션만으로 내가 직접 구현해야했던 offset 계산 및 masking 등을 손 쉽게 해결해준다.\n",
        "\n",
        "- 토크나이저는 디폴트로 문장 시작과 끝에 스페셜 토큰을 붙여주고 paring text도 설정 가능하다. 더불어서 token_type_ids를 통해 offset을 통한 start_index 와 end_index 계산이 가능하다.\n",
        "\n",
        "- 문장간 중간 구분은 separator token으로 이루어져있다.\n",
        "\n",
        "\n",
        "(2)\n",
        "\n",
        "- 이어서 gpt2 style decoder only model을 통해서 Extractive QA task에 대한 학습을 진행하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZNLgYIJC8Lz"
      },
      "source": [
        "(3)\n",
        "- BERT는 GPT와 mha에서 maksing 방식만 다르고 구조 대부분이 동일하다.\n",
        "- 양방향으로 인풋의 각 포인트가 다른 포인트를 참고할 수 있도록 패딩 마스크 이외에 다른 마스크를 사용하지 않는다.\n",
        "- Extractive QA에는 context에서 구조 전체를 볼 수 있는게 더 중요해서 BERT가 더 잘 어울린다고 한다. GPT는 생성형 모델이다.\n",
        "\n",
        "\n",
        "(4)\n",
        "- self supervised learning이란 label이 외부에서 주어지는 것이 아니라 입력 그 자체로부터 자동으로 생성하는 것.\n",
        "\n",
        "- label을 인간이 생성했다는 점에서 출처는 input이 맞아도 extractive qa는 지도학습으로 봐야한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIT6CGcrC-AR"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KT86K1h6AP3j",
        "outputId": "45bef05c-006a-406e-8937-895f590b31e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWoBS1OhAa6W"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "233c43278e2344fea8deddb409346850",
            "270203244a984cbd868ec99092593434",
            "414afa5f100345ed8d0d488b9e67f10e",
            "a285d9b7048d471baf12b3c0194ea564",
            "bae31e4b763a4fc987d5745b5eb399ff",
            "5b1c243c43d54c66b9b704b37455bf76",
            "a25db000053b4fb2a4f253f56a31aca5",
            "dfa0e6d795f846358c1a88935c658708",
            "a130479122dc4fdc917cd1ee709c70a3",
            "caef7cb63ac14efbbd5c1b3b5d500078",
            "2c95be5f04614836ad41d9d66e10f714",
            "460faba46a77485bbac48ac8bb937cd0",
            "25f31b1025de458d984b4037c092fd58",
            "6640b298a9464bc695c39aa183acbbf8",
            "d5bba4d0ff5f4c0aad5d4220782aaa69",
            "aff0aae721c84e8cad9fa1c3b87f9133",
            "a17fab6bc2354a6aaa54592b9852d0dd",
            "dbe898db765544b1b46808054f917172",
            "420af8fc461f459da563751c3f7fa5ec",
            "da507feb0482489fa4677408d0118992",
            "57f9599bcfa54815bfa48aa3d5563134",
            "0c2ff9dc7bd54eeeb989ef0c4b5a7a90",
            "9fa24de38267485281c7d7c1fafaa12a",
            "9f92825ba4814906995d1422043edf0f",
            "7c1e5bfec20d4948b4e09904cb49cdd7",
            "67703b35fef947cdbca390516c1b33e4",
            "78627f920d8b4c8d96676d4d1fb319a6",
            "57ccb2e6735a435aa76b695e4d2849d9",
            "2cda055ae1b8454da7545db445b8a714",
            "929855b77a344309a494fe33c231db6d",
            "89aadc39d8924c1397dbad900e1622b8",
            "3c237d2613464a31b070c4653a209fdb",
            "3cbdb7f8d3a5413b96114b6589025f7b",
            "39568b398c394195abb0b7dd98a72b51",
            "c5a9b432cdfe462b9399083629057cbf",
            "b0104c2b86df4362acb9095fcb30a514",
            "5af1228dac0d4dff8c040224107ff3a7",
            "4ad8bb7f9afa490abdf45cf2471e961f",
            "e8090c69a0ab4b61af72080f72994877",
            "d2e2a8049e2c443c9bbd43ebcb0d9733",
            "f49a786cea194b618077d46ca5fe2936",
            "49778c4f82744955bb2b15ea5098f8f2",
            "3e93d3635b254be29767e67035102a5c",
            "f415cca7e32d4e48915936cec5fe3335",
            "7dca657cb2444ee88b3b1e3a690b7c8f",
            "f3a4830cd3f2405aa52847eb3931fc07",
            "46a42a95f06d4298b3221fe07089133b",
            "991ce053ef7d42259648109cd18df464",
            "5b8b3d7f89254ef4808afad5440a5999",
            "4cd212391133493398018520abb81d80",
            "24a3ee9e39784758a0f2c903132dff64",
            "1935227813404fb7bfe53b8a3ed4b427",
            "9c8fb39c61c64068af6e76baf4fd0d14",
            "80cca4325b964642af92064b2704cbaf",
            "367317672ab64f38abf7edea13fd9f81"
          ]
        },
        "id": "XEkJet9hAbMc",
        "outputId": "682b4d9c-2d06-4416-b4b8-9649871694b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "233c43278e2344fea8deddb409346850",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "460faba46a77485bbac48ac8bb937cd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fa24de38267485281c7d7c1fafaa12a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39568b398c394195abb0b7dd98a72b51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dca657cb2444ee88b3b1e3a690b7c8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data=load_dataset('squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_p1CmBpDIUn"
      },
      "outputs": [],
      "source": [
        "gold_ex=data['train'][0]['answers']['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a22zJRQD1xu"
      },
      "outputs": [],
      "source": [
        "pred = gold_ex + '**'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEj6oOBkDhl7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_answer(s):\n",
        "    def remove_punc(text):\n",
        "        # 영어 구두점 + 한글에 자주 사용되는 구두점 제거\n",
        "        korean_punctuation = '！＂＃＄％＆＇（）＊＋，－．／：；＜＝＞＠［＼］＾＿｀｛｜｝～'  # 한글 특수문자\n",
        "        all_punctuation = string.punctuation + korean_punctuation\n",
        "        return ''.join(ch for ch in text if ch not in all_punctuation)\n",
        "\n",
        "    # 텍스트 정규화\n",
        "    s = remove_punc(s)  # 구두점 제거\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()  # 연속된 공백은 하나로 통합하고 양쪽 공백 제거\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W5_LdcVDpPT"
      },
      "outputs": [],
      "source": [
        "def compute_em(pred, gold):\n",
        "    return int(normalize_answer(pred) == normalize_answer(gold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBMSg_EZDtX8",
        "outputId": "2ada26d9-e7bc-4a1b-ccfe-70d6f24b0e6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_em(gold_ex,pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huKedc3BFskh"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FsKLYXVFkgM",
        "outputId": "95364d53-c73a-4e20-fcc8-72f90fe70f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['사과', '를', '먹', '었', '다', '&', '*', '^', '*', '^', '@@@']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "kkma = Kkma()\n",
        "\n",
        "# 예시 문장 분석\n",
        "text = \"사과를 먹었다&*^*^@@@\"\n",
        "print(kkma.morphs(text))  # ['사과', '를', '먹', '었', '다']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqAhlLOsKsse"
      },
      "outputs": [],
      "source": [
        "refined_answer = normalize_answer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD-5aozaLPCK",
        "outputId": "fd748757-a7d6-439d-c094-ef14f134c593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['사과', '를', '먹', '었', '다']\n"
          ]
        }
      ],
      "source": [
        "print(kkma.morphs(refined_answer))  # ['사과', '를', '먹', '었', '다']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFnwUOSRMjKl"
      },
      "outputs": [],
      "source": [
        "def em(pred,answer):\n",
        "  refined_answer = normalize_answer(answer)\n",
        "  refined_pred = normalize_answer(pred)\n",
        "\n",
        "  analyzed_answer =kkma.morphs(refined_answer)\n",
        "  analyzed_pred =kkma.morphs(refined_pred)\n",
        "\n",
        "  return int(analyzed_answer == analyzed_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOmSx4kxN3Is",
        "outputId": "3da4a706-7413-40e3-897f-c2338314a7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM Score: 1\n"
          ]
        }
      ],
      "source": [
        "answer_text = \"사과를 먹었다\"\n",
        "pred_text = \"사과를 먹었다\"\n",
        "print(f\"EM Score: {em(pred_text, answer_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTUaZoH0N4t_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def f1score(pred,answer):\n",
        "  refined_answer = normalize_answer(answer)\n",
        "  refined_pred = normalize_answer(pred)\n",
        "\n",
        "  analyzed_answer =kkma.morphs(refined_answer)\n",
        "  analyzed_pred =kkma.morphs(refined_pred)\n",
        "\n",
        "  set_answer=Counter(analyzed_answer)\n",
        "  set_pred=Counter(analyzed_pred)\n",
        "\n",
        "  c = set_answer & set_pred\n",
        "  print(set_answer,set_pred,c)\n",
        "  num = sum(c.values())\n",
        "\n",
        "  precision = num / len(analyzed_answer)\n",
        "  recall = num / len(analyzed_pred)\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ulVd1vjQEqM",
        "outputId": "2b4a3453-1eb9-4d24-d9b0-6328f34c6b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'사과': 2, '를': 2, '먹': 2, '고': 1, '또': 1, '었': 1, '다': 1}) Counter({'사과': 1, '먹': 1, '었': 1, '다': 1}) Counter({'사과': 1, '먹': 1, '었': 1, '다': 1})\n",
            "f1 Score: 0.5714285714285715\n"
          ]
        }
      ],
      "source": [
        "answer_text = \"사과를 먹고 또 사과를 먹었다\"\n",
        "pred_text = \"사과 먹었다\"\n",
        "print(f\"f1 Score: {f1score(pred_text, answer_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVezg8aff779",
        "outputId": "9191e93c-53b6-4a0b-c693-6d7368ae823b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive를 /content/drive 경로에 마운트합니다.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpfwPZOuf9b0",
        "outputId": "8d6e0a36-4e94-409f-dad6-c5ec7cc6d99b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ1TC61zQnKX"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT2nlG-HQl6m"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# KOSQUAD 데이터셋 로드\n",
        "# 'squad_kor_v1' 이름을 사용합니다.\n",
        "data = load_dataset(\"squad_kor_v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbvsWRykf-qU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2hZSY8iCnr6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def char_byte_map(text):\n",
        "\n",
        "  char_to_byte = [0]*len(text)\n",
        "\n",
        "  byte_to_char = [0]*len(text.encode('utf-8'))\n",
        "  byte_index = 0\n",
        "  for char_index,char in enumerate(text):\n",
        "\n",
        "    char_to_byte[char_index] = byte_index\n",
        "\n",
        "    length = len(char.encode('utf-8'))\n",
        "\n",
        "    for i in range(length):\n",
        "      byte_to_char[byte_index+i] = char_index\n",
        "\n",
        "    byte_index+=length\n",
        "\n",
        "  return char_to_byte,byte_to_char\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BmG-0eJgQAu"
      },
      "outputs": [],
      "source": [
        "def char_byte_map(text):\n",
        "    char_to_byte = [0] * len(text)\n",
        "    total_bytes = len(text.encode('utf-8'))\n",
        "    # byte_to_char 리스트 크기를 total_bytes + 1로 설정\n",
        "    byte_to_char = [0] * (total_bytes + 1)\n",
        "\n",
        "    byte_index = 0\n",
        "    for char_index, char in enumerate(text):\n",
        "        char_to_byte[char_index] = byte_index\n",
        "\n",
        "        char_bytes = char.encode('utf-8')\n",
        "        length = len(char_bytes)\n",
        "\n",
        "        for i in range(length):\n",
        "            byte_to_char[byte_index + i] = char_index\n",
        "\n",
        "        byte_index += length\n",
        "\n",
        "    # ★ 중요 수정: 마지막 바이트 인덱스 + 1에 해당하는 위치에 총 문자 길이 맵핑\n",
        "    # 이는 end offset 계산 시, 마지막 토큰이 끝나는 바이트 위치 다음의 문자 인덱스를 찾기 위해 필요\n",
        "    byte_to_char[total_bytes] = len(text)\n",
        "\n",
        "    return char_to_byte, byte_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2aeTyq0gOd3"
      },
      "outputs": [],
      "source": [
        "def bytes_to_unicode():\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:] # all integers b in bs will simply map to chr(b) in the output dict\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    d = dict(zip(bs, cs))\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWmf-X1NgGrd"
      },
      "outputs": [],
      "source": [
        "def get_pairs(subwords):\n",
        "  pairs = set()\n",
        "  for i in range(len(subwords)-1):\n",
        "    pair = (subwords[i],subwords[i+1])\n",
        "    pairs.add(pair)\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L1Dx2yuQIfX"
      },
      "outputs": [],
      "source": [
        "def get_file(local_file, remote_file):\n",
        "    \"\"\" downloads remote_file to local_file if necessary \"\"\"\n",
        "    if not os.path.isfile(local_file):\n",
        "        print(f\"downloading {remote_file} to {local_file}\")\n",
        "        response = requests.get(remote_file)\n",
        "        open(local_file, \"wb\").write(response.content)\n",
        "\n",
        "def get_encoder():\n",
        "    \"\"\"\n",
        "    Returns an instance of the GPT BPE Encoder/Decoder\n",
        "    and handles caching of \"database\" files.\n",
        "    \"\"\"\n",
        "    home_dir = './'\n",
        "\n",
        "\n",
        "\n",
        "    # load encoder.json that has the raw mappings from token -> bpe index\n",
        "    encoder_local_file = os.path.join('./', 'encoder.json')\n",
        "    encoder_remote_file = 'https://huggingface.co/kakaobrain/kogpt/resolve/main/tokenizer/encoder.json'\n",
        "    get_file(encoder_local_file, encoder_remote_file)\n",
        "    with open(encoder_local_file, 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    assert len(encoder) == 50257 # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token\n",
        "\n",
        "    # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure\n",
        "    # in the form tuples (a, b), that indicate that (a, b) is to be merged to one token ab\n",
        "    vocab_local_file = os.path.join('./', 'vocab.bpe')\n",
        "    vocab_remote_file = 'https://huggingface.co/kakaobrain/kogpt/resolve/main/tokenizer/vocab.bpe'\n",
        "    get_file(vocab_local_file, vocab_remote_file)\n",
        "    with open(vocab_local_file, 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    # light postprocessing: strip the version on first line and the last line is a blank\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    assert len(bpe_merges) == 50000 # 50,000 merged tokens\n",
        "\n",
        "    # construct the Encoder object and return\n",
        "\n",
        "    return encoder,bpe_merges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpEsMJiGf2qv"
      },
      "outputs": [],
      "source": [
        "encoder,bpe_merges=get_encoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j3_2TzYgkah"
      },
      "outputs": [],
      "source": [
        "class Encoder :\n",
        "  def __init__(self,encoder,bpe_merge_rule):\n",
        "\n",
        "    self.byte_encoder = bytes_to_unicode()\n",
        "    self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}\n",
        "    self.encoder = encoder\n",
        "    self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "\n",
        "    self.bpe_ranks = dict(zip(bpe_merge_rule,range(len(bpe_merge_rule))))\n",
        "\n",
        "    self.pat = re.compile(r\"[\\p{IsHangul}]+| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\")\n",
        "    self.cache = {}\n",
        "\n",
        "  def bpe(self,unicode_pre_token):\n",
        "\n",
        "    word = tuple(unicode_pre_token)\n",
        "    pairs = get_pairs(word)\n",
        "\n",
        "    while pairs :\n",
        "\n",
        "      pair = min(pairs,key = lambda pair:self.bpe_ranks.get(pair,float('inf')))\n",
        "      first,second = pair\n",
        "      if pair not in self.bpe_ranks :\n",
        "        break\n",
        "      new_word = []\n",
        "      i = 0\n",
        "      while i < len(word):\n",
        "        if i < len(word)-1 and word[i] == first and word[i+1] == second:\n",
        "          new_word.append(word[i]+word[i+1])\n",
        "          i+=2\n",
        "        else:\n",
        "          new_word.append(word[i])\n",
        "          i+=1\n",
        "\n",
        "      word = new_word\n",
        "      pairs = get_pairs(word)\n",
        "\n",
        "    return ' '.join(word)\n",
        "\n",
        "  def encoder_with_offsets(self,text):\n",
        "    output_index = []\n",
        "    offsets = []\n",
        "    char_to_byte,byte_to_char = char_byte_map(text)\n",
        "    for match in re.finditer(self.pat,text):\n",
        "      pre_token = match.group(0)\n",
        "      start_char_index_in_text = match.start()\n",
        "      start_byte_index_in_text = char_to_byte[start_char_index_in_text]\n",
        "      encoded_pre_token = pre_token.encode('utf-8')\n",
        "\n",
        "      unicode_pre_token = ''.join([self.byte_encoder[b] for b in encoded_pre_token])\n",
        "      subwords = self.bpe(unicode_pre_token).split(' ')\n",
        "\n",
        "      byte_cursor = 0  # encoded_pre_token 기준\n",
        "      for subword in subwords:\n",
        "          # subword를 디코딩해서 실제 바이트 길이를 얻음\n",
        "          subword_bytes = bytes([self.byte_decoder[c] for c in subword])\n",
        "          subword_len = len(subword_bytes)\n",
        "\n",
        "          # 전체 text에서 이 subword의 시작 byte index\n",
        "          global_byte_index = start_byte_index_in_text + byte_cursor\n",
        "\n",
        "          local_char_index = byte_to_char[global_byte_index]\n",
        "\n",
        "          char_end_index = byte_to_char[global_byte_index+subword_len] if global_byte_index+subword_len < len(byte_to_char) else len(text)\n",
        "\n",
        "          offsets.append((local_char_index,char_end_index))\n",
        "          output_index.append(self.encoder[subword])\n",
        "\n",
        "          byte_cursor += subword_len  # 다음 subword 위치로 이동\n",
        "    return output_index,offsets\n",
        "\n",
        "  def decode(self,index):\n",
        "\n",
        "    joined_subwords =''.join([self.decoder[i] for i in index])\n",
        "\n",
        "    decoded = bytearray([self.byte_decoder[c] for c in joined_subwords])\n",
        "\n",
        "\n",
        "    text=decoded.decode('utf-8',errors = 'replace')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYCjRdz6k5J4"
      },
      "outputs": [],
      "source": [
        "enc = Encoder(encoder,bpe_merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50vTnM_JEzos",
        "outputId": "59f3dbba-0c27-4761-96c8-2a4c67784d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835,
          "referenced_widgets": [
            "ba0bcb21926846eb9c08a6e36ea74502",
            "ac97ffee79e54385af844bc1250b2054",
            "fd904eb1d99e4655b8c0584f98c5515e",
            "925e1b1b519241bf87c82ef99f274be6",
            "a3f1a3e2e208457cbcb6d330616f1727",
            "1bfff1752f9748578eaa7f3538dedf27",
            "9dbab810dbd54da8a985c30b4274f439",
            "46800170dd7c4d78bc9589fa98df5f57",
            "fae5b38561a9411d8a954c8287ded932",
            "12568e61a3a741c2805a0ba39dbf7280",
            "872574dab4e945fabc772ea5fda38e9c",
            "2528a055b55c4e18b8925589109bf708",
            "f57e77c321fc414bb1daf4051a21745a",
            "a92c571bd02b44f587e4c117c6cd841b",
            "0ff86b93f0f444bc83ace2541221036b",
            "370506365cd140e2b12a84a96b811ff8",
            "d6a0a0e4977945a899304cc11487567f",
            "c991822bf6b44385a9d5700a0bb7c80f",
            "3f20f2384b004c539797bbb7d61780d8",
            "9697069c6d9a4585b40b88ed30a8774a",
            "a607090744584ec889d69e389003895e",
            "1b06bb72bedc4ab78c7ab79178df2fac",
            "cd57bc78e1474979adf1c30fe52eb177",
            "91685819acb44d409e2e5b44c4fd4878",
            "415e64b5d3314e198efddd96bebb3a48",
            "54412d2e96e043bc91b5b5d46b681ad0",
            "917cdc1cbf32447390a78a7a6921ba7f",
            "e33460baa05645e397e648cbce669378",
            "431ca8e972984f7387f3ea05026e1d1f",
            "b06ad2b074494ae9ad3bb1e2320fbfad",
            "bea510419743490abd772822fd0f4b43",
            "5ffc82fa41a44df6b14c58818933e34b",
            "4539ab71f2fd4f778af6a96c64cfbf9b",
            "c8f4aba0ed3540c186d9472b1aae0093",
            "1f85f3e4e24841c68b1a3b5bdec995ab",
            "6c3184397a2247ed89a852cb101b7fa5",
            "42d1362da39b47fca3e46ad87cbc2077",
            "e66324a4f79f42da8c794b62197d7447",
            "351de73f2fe64dceb243f306c3835144",
            "7b126693557a4cb8b3fccd63413ed6ec",
            "9da2e6f4e57c4903be98b9311deaaf45",
            "90c1fc1d33454ac8b5a17be52cee1a5d",
            "9c0d9d3181ff460e89362cc1678a9eda",
            "a991c22829bd49a28e7f3b6b7c689cde"
          ]
        },
        "id": "rbYqRJmrE0xH",
        "outputId": "e1c4dd0c-d713-4c12-ff90-a607103ce4bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba0bcb21926846eb9c08a6e36ea74502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2528a055b55c4e18b8925589109bf708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd57bc78e1474979adf1c30fe52eb177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8f4aba0ed3540c186d9472b1aae0093",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is the loaded tokenizer fast? True\n",
            "Input IDs: tensor([[    0,  5891,  2205,  5971,    16, 12235, 14766, 16023, 15743,    53,\n",
            "          2106,  7453, 12190,    18,     2]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Offsets: tensor([[[ 0,  0],\n",
            "         [ 0,  2],\n",
            "         [ 2,  3],\n",
            "         [ 3,  5],\n",
            "         [ 5,  6],\n",
            "         [ 7,  9],\n",
            "         [ 9, 11],\n",
            "         [11, 14],\n",
            "         [14, 17],\n",
            "         [18, 19],\n",
            "         [19, 20],\n",
            "         [21, 24],\n",
            "         [24, 27],\n",
            "         [27, 28],\n",
            "         [ 0,  0]]])\n",
            "\n",
            "Tokens with Offsets:\n",
            "'[CLS]': [0, 0] -> ''\n",
            "'안녕': [0, 2] -> '안녕'\n",
            "'##하': [2, 3] -> '하'\n",
            "'##세요': [3, 5] -> '세요'\n",
            "',': [5, 6] -> ','\n",
            "'ex': [7, 9] -> 'ex'\n",
            "'##tr': [9, 11] -> 'tr'\n",
            "'##act': [11, 14] -> 'act'\n",
            "'##ive': [14, 17] -> 'ive'\n",
            "'Q': [18, 19] -> 'Q'\n",
            "'##A': [19, 20] -> 'A'\n",
            "'테스트': [21, 24] -> '테스트'\n",
            "'##입니다': [24, 27] -> '입니다'\n",
            "'.': [27, 28] -> '.'\n",
            "'[SEP]': [0, 0] -> ''\n",
            "\n",
            "토큰 인덱스 [5, 8]에 해당하는 원본 텍스트 스팬: 'extractive'\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch # return_tensors='pt'를 위해 필요\n",
        "\n",
        "text = \"안녕하세요, extractive QA 테스트입니다.\"\n",
        "\n",
        "# 사용할 모델의 이름을 klue/roberta-base 로 변경 ★★★\n",
        "# use_fast=True 옵션 유지 ★★★\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\", use_fast=True)\n",
        "\n",
        "# 로드된 토크나이저가 빠른 토크나이저인지 확인해봅니다.\n",
        "print(f\"Is the loaded tokenizer fast? {tokenizer.is_fast}\") # True가 나와야 합니다.\n",
        "\n",
        "\n",
        "# 토큰화 수행 시 offset_mapping=True 옵션을 설정\n",
        "encoded = tokenizer(text, return_offsets_mapping=True, return_tensors='pt')\n",
        "\n",
        "# 결과 확인\n",
        "input_ids = encoded['input_ids']\n",
        "# KLUE-RoBERTa는 BERT와 달리 token_type_ids를 사용하지 않습니다.\n",
        "attention_mask = encoded['attention_mask']\n",
        "offsets = encoded['offset_mapping'] # <-- 이제 이 정보가 정상적으로 반환될 것입니다.\n",
        "\n",
        "print(\"Input IDs:\", input_ids)\n",
        "print(\"Attention Mask:\", attention_mask)\n",
        "# print(\"Token Type IDs:\", token_type_ids) # KLUE-RoBERTa에서는 없음\n",
        "print(\"Offsets:\", offsets)\n",
        "\n",
        "# 오프셋 정보를 이용해 토큰과 원본 텍스트 매핑 확인\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "\n",
        "print(\"\\nTokens with Offsets:\")\n",
        "for token, offset in zip(tokens, offsets[0].tolist()):\n",
        "    if offset is not None:\n",
        "         original_span = text[offset[0]:offset[1]]\n",
        "         print(f\"'{token}': {offset} -> '{original_span}'\")\n",
        "    else:\n",
        "         print(f\"'{token}': {offset}\")\n",
        "\n",
        "# 예시: 임의의 토큰 인덱스 범위를 선택하여 원본 텍스트 스팬 추출\n",
        "# (위 print 결과의 토큰/오프셋을 보고 유효한 범위를 지정해주세요)\n",
        "# 여기서는 예시로 인덱스 5부터 8까지를 사용합니다.\n",
        "predicted_start_token = 5\n",
        "predicted_end_token = 8\n",
        "\n",
        "if 5 < len(tokens) and 8 < len(tokens):\n",
        "    # 예측된 토큰 인덱스에 해당하는 원본 텍스트 스팬 추출\n",
        "    # 시작 토큰의 시작 오프셋 (offset[predicted_start_token][0])\n",
        "    # 끝 토큰의 끝 오프셋 (offset[predicted_end_token][1])\n",
        "    start_char = offsets[0][predicted_start_token][0].item()\n",
        "    end_char = offsets[0][predicted_end_token][1].item()\n",
        "\n",
        "    predicted_answer_span = text[start_char:end_char]\n",
        "    print(f\"\\n토큰 인덱스 [{predicted_start_token}, {predicted_end_token}]에 해당하는 원본 텍스트 스팬: '{predicted_answer_span}'\")\n",
        "else:\n",
        "     print(\"\\n예시 토큰 인덱스 [5, 8]이 유효한 범위를 벗어납니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gidahNAOwLi"
      },
      "outputs": [],
      "source": [
        "class config :\n",
        "  max_length = 256\n",
        "  batch_size = 32\n",
        "  hidden_dim = 512\n",
        "  num_heads = 8\n",
        "  dropout_rate = 0.1\n",
        "  ffn_dim = 512*4\n",
        "  num_layers = 6\n",
        "  voca_size = tokenizer.vocab_size\n",
        "  padding_id = tokenizer.pad_token_id\n",
        "  lr = 1e-5\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H5WWPQdQMZ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer # Hugging Face 토크나이저 사용 가정\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenizer: AutoTokenizer, full_text: list, config, is_training: bool = True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.full_text = full_text\n",
        "        self.config = config\n",
        "        self.is_training = is_training\n",
        "        self.max_length = config.max_length # 모델의 최대 길이\n",
        "        # self.padding_id = config.padding_id # 토크나이저에서 직접 가져옵니다.\n",
        "        self.padding_id = self.tokenizer.pad_token_id # 토크나이저의 패딩 ID\n",
        "\n",
        "        self.features = [] # 처리된 특징(feature)들을 저장할 리스트\n",
        "\n",
        "        # 필요한 스페셜 토큰 ID 미리 가져오기 (RoBERTa의 경우 <s>, </s>)\n",
        "        self.cls_token_id = self.tokenizer.cls_token_id # BERT의 [CLS], RoBERTa의 <s>\n",
        "        self.sep_token_id = self.tokenizer.sep_token_id # BERT의 [SEP], RoBERTa의 </s>\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id # 패딩 토큰 ID\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def _build(self):\n",
        "        # 각 샘플(질문/지문/답변 쌍)을 순회하며 특징 생성\n",
        "        for a_sample in self.full_text:\n",
        "            q = a_sample['question']\n",
        "            c = a_sample['context']\n",
        "            # 학습 시에는 답변 정보가 필요합니다. 추론 시에는 필요 없거나 예측 후 비교용.\n",
        "            # 여기서는 학습을 가정하고 답변 정보를 가져옵니다.\n",
        "            answer_text = a_sample['answers']['text'][0]\n",
        "            answer_start_in_context = a_sample['answers']['answer_start'][0] # 원본 context 텍스트 내에서의 시작 문자 인덱스\n",
        "            answer_end_in_context = answer_start_in_context + len(answer_text) # 원본 context 텍스트 내에서의 끝 문자 인덱스 (exclusive)\n",
        "\n",
        "            encoded = self.tokenizer(\n",
        "                q, # 첫 번째 시퀀스 (질문)\n",
        "                text_pair=c, # 두 번째 시퀀스 (지문)\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_offsets_mapping=True, # 토큰별 원본 텍스트 문자 오프셋\n",
        "                return_attention_mask=True, # 어텐션 마스크\n",
        "                # RoBERTa는 token_type_ids를 사용하지 않습니다. BERT 사용 시 필요.\n",
        "                # return_token_type_ids=True # BERT 사용 시 추가\n",
        "            )\n",
        "\n",
        "\n",
        "            input_ids = encoded['input_ids']\n",
        "            attention_mask = encoded['attention_mask']\n",
        "            offset_mapping = encoded['offset_mapping']\n",
        "\n",
        "            second_sep_token_index = -1\n",
        "\n",
        "            for i,flag in enumerate(encoded.sequence_ids()):\n",
        "              if flag is not None and flag == 1 :\n",
        "                second_sep_token_index = i\n",
        "                break\n",
        "            # 컨텍스트 시작 토큰의 인덱스는 그 다음 토큰입니다.\n",
        "            context_start_token_index = second_sep_token_index\n",
        "\n",
        "\n",
        "            # ★★★ 답변의 시작/끝 토큰 인덱스 찾기 ★★★\n",
        "            # 로드한 데이터셋의 answer_start_in_context (원본 context 텍스트 내에서의 문자 인덱스)와\n",
        "            # 토크나이저의 offset_mapping (토큰별 원본 텍스트 문자 오프셋)을 비교하여 찾습니다.\n",
        "            # 답변은 지문(context) 내에 있습니다.\n",
        "\n",
        "            start_token_idx = -1 # 찾은 시작 토큰 인덱스 (못 찾으면 -1)\n",
        "            end_token_idx = -1   # 찾은 끝 토큰 인덱스 (못 찾으면 -1)\n",
        "            for i in range(self.max_length):\n",
        "                if offset_mapping[i] == (-1, -1):\n",
        "                    continue\n",
        "\n",
        "                token_char_start, token_char_end = offset_mapping[i] # 토큰 i의 원본 텍스트 문자 오프셋 (start, end)\n",
        "                is_context_token = (i >= context_start_token_index)\n",
        "\n",
        "\n",
        "                if is_context_token:\n",
        "                    if token_char_start <= answer_start_in_context < token_char_end:\n",
        "                         if start_token_idx == -1: # 첫 번째 일치하는 토큰만 시작 토큰으로 설정\n",
        "                              start_token_idx = i\n",
        "                    if token_char_start <= answer_end_in_context - 1 < token_char_end:\n",
        "                         # 마지막 일치하는 토큰이 끝 토큰이 되도록 계속 업데이트\n",
        "                         end_token_idx = i\n",
        "\n",
        "            is_valid_answer_span = False\n",
        "            if start_token_idx != -1 and end_token_idx != -1 and start_token_idx <= end_token_idx:\n",
        "                 # 추가 검증: 찾은 토큰 범위에 해당하는 원본 텍스트 스팬이 실제 답변 텍스트와 일치하는지 확인\n",
        "                 # (이 검증은 정확성을 높이지만, 토크나이저의 미묘한 차이로 인해 불일치가 발생하기도 합니다. 선택 사항입니다.)\n",
        "                 # 찾은 토큰 범위의 문자 오프셋: 시작 토큰의 시작 문자 오프셋 ~ 끝 토큰의 끝 문자 오프셋\n",
        "                 predicted_ans_start_char = offset_mapping[start_token_idx][0]\n",
        "                 predicted_ans_end_char = offset_mapping[end_token_idx][1] # exclusive\n",
        "\n",
        "                 # 이 오프셋은 원본 컨텍스트 기준이므로, 원본 c에서 추출합니다.\n",
        "                 extracted_text = c[predicted_ans_start_char:predicted_ans_end_char]\n",
        "\n",
        "                 if extracted_text == answer_text:\n",
        "                      is_valid_answer_span = True\n",
        "                 # else:\n",
        "                     # print(f\"Warning: Extracted text '{extracted_text}' from tokens [{start_token_idx}, {end_token_idx}] does not match ground truth '{answer_text}'.\")\n",
        "\n",
        "\n",
        "            # 유효한 답변 스팬을 찾지 못했거나 학습 모드인데 답변이 없는 경우 스킵\n",
        "            # SQuAD v1.1은 모든 질문에 답변이 있으므로, 찾지 못했다면 문제가 있거나 길이가 잘린 경우입니다.\n",
        "            if self.is_training and not is_valid_answer_span:\n",
        "                 # 해당 학습 샘플은 제외합니다.\n",
        "                 # print(f\"Skipping training sample {a_sample['id']}: Answer span not found or invalid.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            # ★★★ 특징(Feature) 저장 ★★★\n",
        "            # 모델 입력에 필요한 정보들을 저장합니다.\n",
        "            feature = {\n",
        "                'input_ids': input_ids, # 토큰 ID 시퀀스 (패딩/잘림 완료)\n",
        "                'attention_mask': attention_mask, # 어텐션 마스크 (패딩 반영)\n",
        "                'offset_mapping': offset_mapping, # 토큰별 원본 텍스트 문자 오프셋\n",
        "                'question_id': a_sample['id'], # SQuAD 평가에 필요\n",
        "                'answer_text': answer_text, # 검증/디버깅에 유용 (추론 시에는 없거나 임시값)\n",
        "                'context': c, # 검증/디버깅에 유용 (추론 시에는 원본 지문 제공)\n",
        "                # 'token_type_ids': token_type_ids # BERT 사용 시 추가\n",
        "            }\n",
        "\n",
        "            # 학습 시에만 정답 시작/끝 토큰 인덱스를 저장\n",
        "            if self.is_training:\n",
        "                 if is_valid_answer_span:\n",
        "                      feature['start_positions'] = start_token_idx\n",
        "                      feature['end_positions'] = end_token_idx\n",
        "                 # else: # is_valid_answer_span이 False인 경우, 이미 continue로 스킵됨\n",
        "\n",
        "\n",
        "            self.features.append(feature)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # DataLoader가 호출할 때 실제 학습/추론에 사용할 PyTorch 텐서 형태로 변환\n",
        "        feature = self.features[idx]\n",
        "        item = {\n",
        "            'input_ids': torch.as_tensor(feature['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.as_tensor(feature['attention_mask'], dtype=torch.long),\n",
        "            # 'offset_mapping'은 모델 입력 자체에는 필요 없지만, 추론 후 원본 텍스트 복원에 필요합니다.\n",
        "            # 따라서 DataLoader의 반환값에 포함시켜 추론 스크립트에서 사용합니다.\n",
        "            'offset_mapping': torch.as_tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "            'question_id': feature['question_id'],\n",
        "            'answer_text': feature['answer_text'], # 검증/디버깅용\n",
        "            'context': feature['context'] # 검증/디버깅용\n",
        "            # 'token_type_ids': torch.as_tensor(feature['token_type_ids'], dtype=torch.long) if 'token_type_ids' in feature else None,\n",
        "        }\n",
        "        # 학습 시에만 정답 레이블 포함\n",
        "        if self.is_training:\n",
        "            item['start_positions'] = torch.as_tensor(feature['start_positions'], dtype=torch.long)\n",
        "            item['end_positions'] = torch.as_tensor(feature['end_positions'], dtype=torch.long)\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t44wAlbwQoGd"
      },
      "outputs": [],
      "source": [
        "del data_inst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd0gbNQ0QP1a"
      },
      "outputs": [],
      "source": [
        "data_inst=QADataset(tokenizer,data['train'],config,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT0DMCoXpVoS"
      },
      "outputs": [],
      "source": [
        "torch.save(data_inst.features,'./kosquad_train_features.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53fHB6vApY9O"
      },
      "outputs": [],
      "source": [
        "class obs(torch.utils.data.Dataset):\n",
        "  def __init__(self,config,is_training = True):\n",
        "    self.is_training =is_training\n",
        "    self.features = torch.load('./kosquad_train_features.pt')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  def __getitem__(self, idx):\n",
        "      # DataLoader가 호출할 때 실제 학습/추론에 사용할 PyTorch 텐서 형태로 변환\n",
        "      feature = self.features[idx]\n",
        "      item = {\n",
        "          'input_ids': torch.as_tensor(feature['input_ids'], dtype=torch.long),\n",
        "          'attention_mask': torch.as_tensor(feature['attention_mask'], dtype=torch.long),\n",
        "          # 'offset_mapping'은 모델 입력 자체에는 필요 없지만, 추론 후 원본 텍스트 복원에 필요합니다.\n",
        "          # 따라서 DataLoader의 반환값에 포함시켜 추론 스크립트에서 사용합니다.\n",
        "          'offset_mapping': torch.as_tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "          'question_id': feature['question_id'],\n",
        "          'answer_text': feature['answer_text'], # 검증/디버깅용\n",
        "          'context': feature['context'] # 검증/디버깅용\n",
        "          # 'token_type_ids': torch.as_tensor(feature['token_type_ids'], dtype=torch.long) if 'token_type_ids' in feature else None,\n",
        "      }\n",
        "      # 학습 시에만 정답 레이블 포함\n",
        "      if self.is_training:\n",
        "          item['start_positions'] = torch.as_tensor(feature['start_positions'], dtype=torch.long)\n",
        "          item['end_positions'] = torch.as_tensor(feature['end_positions'], dtype=torch.long)\n",
        "\n",
        "      return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STaCAJfvsSD2",
        "outputId": "2e4bc699-c3a2-4aca-e3c0-3582529e7a02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "GqpvgmIqrlcf",
        "outputId": "1e14136b-51dd-40b0-c5fd-bd412389b9dc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_inst' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-4c25bc08d48b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdata_inst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data_inst' is not defined"
          ]
        }
      ],
      "source": [
        "del data_inst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mH9WdoYppK9"
      },
      "outputs": [],
      "source": [
        "o = obs(config,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gghgiAbTSrql"
      },
      "outputs": [],
      "source": [
        "loader=torch.utils.data.DataLoader(o,batch_size = config.batch_size,shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw60-NZhSwpJ",
        "outputId": "b9bca3f9-81d3-46f7-d216-e1e2f74bf7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    0, 10665,  2440,  ...,     1,     1,     1],\n",
            "        [    0,  3665,  3648,  ...,  7285,  5009,     2],\n",
            "        [    0, 24185,  2079,  ...,  2079, 18016,     2],\n",
            "        ...,\n",
            "        [    0, 10062, 11369,  ...,  2200,  3666,     2],\n",
            "        [    0,  1421,  2075,  ...,     1,     1,     1],\n",
            "        [    0,  8673,  2151,  ...,     1,     1,     1]])\n",
            "tensor([211, 110,  89,  42,  16,  59,  33,  59, 171,  59,  93, 155, 150, 153,\n",
            "         45, 107,  41, 189, 211,  42,  47, 149,  61,  97, 132, 181, 206, 250,\n",
            "         70, 187, 146,  75])\n",
            "tensor([215, 110,  93,  48,  18,  60,  35,  64, 172,  59,  94, 156, 150, 154,\n",
            "         47, 108,  43, 189, 212,  44,  52, 151,  64,  98, 135, 182, 209, 252,\n",
            "         70, 189, 149,  77])\n"
          ]
        }
      ],
      "source": [
        "for sample in loader :\n",
        "  print(sample['input_ids'])\n",
        "  print(sample['start_positions'])\n",
        "  print(sample['end_positions'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXk9sEzcS1wG"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = config.hidden_dim\n",
        "    self.num_heads = config.num_heads\n",
        "    self.a_head_dim = self.hidden_dim // self.num_heads\n",
        "\n",
        "    self.Linear_Q = torch.nn.Linear(config.hidden_dim,config.hidden_dim)\n",
        "    self.Linear_K = torch.nn.Linear(config.hidden_dim,config.hidden_dim)\n",
        "    self.Linear_V = torch.nn.Linear(config.hidden_dim,config.hidden_dim)\n",
        "    self.dropout = torch.nn.Dropout(config.dropout_rate)\n",
        "    self.Linear_out = torch.nn.Linear(config.hidden_dim,config.hidden_dim)\n",
        "    for name, param in self.named_parameters():\n",
        "      if 'weight' in name:\n",
        "        torch.nn.init.xavier_uniform_(param)\n",
        "      elif 'bias' in name:\n",
        "        torch.nn.init.zeros_(param)\n",
        "\n",
        "  def forward(self,query,key,value,padding_mask,device):\n",
        "    B,T,S = query.size(0),query.size(1),key.size(1)\n",
        "    H,NH,aH = self.hidden_dim,self.num_heads,self.a_head_dim\n",
        "    #print(\"query mean/std:\", query.mean().item(), query.std().item())\n",
        "    q = self.Linear_Q(query)\n",
        "    k = self.Linear_K(key)\n",
        "    v = self.Linear_V(value)\n",
        "\n",
        "    q = q.view(B,T,NH,aH).permute(0,2,1,3)\n",
        "    k = k.view(B,S,NH,aH).permute(0,2,1,3)\n",
        "    v = v.view(B,S,NH,aH).permute(0,2,1,3)\n",
        "\n",
        "    qk = torch.matmul(q,k.transpose(3,2)) / math.sqrt(aH)\n",
        "\n",
        "    #print(\"qk mean:\", qk.mean().item())\n",
        "    #print(\"qk std :\", qk.std().item())\n",
        "\n",
        "    # key 마스킹\n",
        "    key_mask = padding_mask.unsqueeze(1).unsqueeze(2).expand(B, 1, T, S)  # (B, 1, T, S)\n",
        "    #print(padding_mask.sum(dim = -1))\n",
        "    qk = qk.masked_fill(key_mask == 0, float('-inf'))\n",
        "\n",
        "    # query 마스킹은 softmax 후 곱셈으로 (안전하게)\n",
        "    #query_mask = padding_mask.unsqueeze(1).unsqueeze(3).expand(B, 1, T, 1)  # (B, 1, T, 1)\n",
        "\n",
        "    mask = torch.zeros((T,S),device = device)\n",
        "    tmp = torch.ones((T,S),dtype = torch.bool,device = device).tril(diagonal=0)\n",
        "    mask.masked_fill_(tmp.logical_not(),float('-inf'))\n",
        "\n",
        "    qk += mask\n",
        "    invalid_mask = torch.isinf(qk).all(dim=-1)\n",
        "    if invalid_mask.any():\n",
        "      print(\"⚠️ Invalid attention row detected at:\", invalid_mask.nonzero())\n",
        "    #print(torch.sum(qk == float('-inf'),dim = -1))\n",
        "\n",
        "    #qk = self.dropout(qk)\n",
        "    alpha = torch.softmax(qk,dim = -1)\n",
        "\n",
        "    #alpha = alpha * query_mask.float()  # query가 padding인 위치는 attention score를 0으로\n",
        "    #alpha = alpha * key_mask.float()\n",
        "    alpha = self.dropout(alpha)\n",
        "    #print(\"alpha mean:\", alpha.mean().item())\n",
        "    #print(\"alpha std:\", alpha.std().item())\n",
        "\n",
        "    scores = torch.matmul(alpha,v)\n",
        "    scores = scores.permute(0,2,1,3).reshape(B,T,H)\n",
        "\n",
        "    output = self.Linear_out(scores)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WkOPpbCohS-"
      },
      "outputs": [],
      "source": [
        "class NewGELU(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YFwW5AfojSV"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.mha_layer_norm = torch.nn.LayerNorm(config.hidden_dim)\n",
        "    self.mha = MultiHeadAttention(config)\n",
        "    self.mha_dropout = torch.nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    self.ffn_layer_norm = torch.nn.LayerNorm(config.hidden_dim)\n",
        "    self.ffn = torch.nn.Linear(config.hidden_dim,config.ffn_dim)\n",
        "    self.act = NewGELU()\n",
        "    self.rev_ffn = torch.nn.Linear(config.ffn_dim,config.hidden_dim)\n",
        "    self.ffn_dropout = torch.nn.Dropout(config.dropout_rate)\n",
        "\n",
        "  def _mha(self,src,padding_mask):\n",
        "    return self.mha_dropout(self.mha(src,src,src,padding_mask,self.config.device))\n",
        "\n",
        "  def _ffn(self,mha_output):\n",
        "    inter = self.act(self.ffn(mha_output))\n",
        "    return self.ffn_dropout(self.rev_ffn(inter))\n",
        "\n",
        "  def forward(self,src,padding_mask):\n",
        "    x = src\n",
        "\n",
        "    x = self.mha_layer_norm(x + self._mha(x,padding_mask))\n",
        "    return self.ffn_layer_norm(x+ self._ffn(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Q4KIo6ok70"
      },
      "outputs": [],
      "source": [
        "def get_modules(module,num_layers):\n",
        "  return torch.nn.ModuleList([module for _ in range(num_layers)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G23wrDTsomkw"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.layers = get_modules(DecoderLayer(config),config.num_layers)\n",
        "\n",
        "  def forward(self,src,padding_mask):\n",
        "    output = src\n",
        "    for layer in self.layers :\n",
        "      output = layer(output,padding_mask)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRl1Bsvnon-Z"
      },
      "outputs": [],
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(config.voca_size,config.hidden_dim)\n",
        "    self.pos = torch.nn.Embedding(config.max_length,config.hidden_dim)\n",
        "    self.block = DecoderBlock(config)\n",
        "    self.Layer_norm = torch.nn.LayerNorm(config.hidden_dim)\n",
        "    self.Linear_start = torch.nn.Linear(config.hidden_dim,1)\n",
        "    self.Linear_end = torch.nn.Linear(config.hidden_dim,1)\n",
        "\n",
        "  def forward(self,src,padding_mask):\n",
        "    T = src.size(1)\n",
        "    embedded = self.embedding(src)\n",
        "\n",
        "    seq=torch.arange(T,dtype = torch.long,device = self.config.device)\n",
        "    pos_embedding = self.pos(seq)\n",
        "    embedded_src = embedded + pos_embedding\n",
        "    output=self.block(embedded_src,padding_mask)\n",
        "    output = self.Layer_norm(output)\n",
        "    start_output=self.Linear_start(output)\n",
        "    end_output=self.Linear_end(output)\n",
        "\n",
        "    return start_output.squeeze(-1),end_output.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr3X-1Kfop3J"
      },
      "outputs": [],
      "source": [
        "tr=Transformer(config)\n",
        "tr = tr.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrgF-qijorlR"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(tr.parameters(),lr = config.lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV0c2aCEotO9"
      },
      "outputs": [],
      "source": [
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J01nDVSlou2-"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j8XKEroowHQ"
      },
      "outputs": [],
      "source": [
        "valid_data=QADataset(tokenizer,data['validation'],config,True)\n",
        "val_loader=torch.utils.data.DataLoader(valid_data,batch_size = config.batch_size,shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2tS_GUCoxhr"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7dibyXEo2cB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TnqMiHhtf9D",
        "outputId": "865573d2-7843-402b-8de8-ad389463362d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'question_id', 'answer_text', 'context', 'start_positions', 'end_positions'])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynF5xdQrto6e"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec_YNBktwt3O"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414,
          "referenced_widgets": [
            "48d539e070c645ef93ba88b926b4f5a3",
            "0f22c73e16f946e5a60b98d29abcedde",
            "78848f5cde3d4bafb20fc241edf0e0eb",
            "827b3a52ad684e2ba06e6caacbe7ae93",
            "09613fe84b6d432592cc53b4a6874b90",
            "4da60dd93fa247429d84d4f02f236998",
            "b2619195700b4e34a165eb9679a1f702",
            "cfb2f29b3d514585b864df1a90960342",
            "b36fc59d71214642bccd2a1250ef8657",
            "ad2a199b24ec45ddb5010aab61801bf1",
            "57e28259c0cd4cf9b57192a97182e0ca",
            "751626604a814aa5971397d2865c627f",
            "929debdfe83c46f698648c252714e8ab",
            "ce852a2c5deb4955bad2c399108a3bc6",
            "60f28f7178094eb29364f23013d853a5",
            "3d73830aa1de4ae0a5964d361f39e765",
            "25f872c3b53b4d1a8172c63c8f042ccc",
            "4dc37de723eb4f9faacab8f75c412bfe",
            "9099175c6ef94551ae833d9287d9c8a7",
            "c4bef9da21414c83a21b1efe6b6a9056",
            "369094debdd84b62af6f38364a031ec9",
            "0eb32787054148d08efc4e21e45ef7cf",
            "346fd6c7b39f4f4b8750205207da2e95",
            "9a65a02ec7854d33b2a838fcadbacf6a",
            "f423b691a640416da99f41b5c3753d34",
            "b872ed1e08984a919edbf8160895f4df",
            "ad397e0dac2f4d21b770d2c5a2cf5932",
            "f31aab4bb00d4b30b819261e76e6cf3a",
            "ca215f80993542dc96c58edad0c0cc5a",
            "a72e08e994c14a7583ad8582a7535856",
            "b96950586ff246c8bb357afae9b6270e",
            "04ecb3e51d2e413da07f4049c02d99df",
            "c5369a43ac6e45e3a7e8c9888a36f5d0",
            "c0de2d21a7b948e3b528720aeb3cc9eb",
            "f8646452c2104c8fa7f172e2dd8b1662",
            "97400896ec174b61af76bf39c404d140",
            "f94b873622464ce0803dbfd7fbed9e23",
            "4cedfae6d60a46d29e4faecaeadca510",
            "42ee323b41ed4f00b42b3588c469f561",
            "b27d022296944d52ac9a09288d2fc192",
            "e9bd4eddfbc54e43bfc37423b9cf4e11",
            "7c649a7af3534c418f2e6e4bff120d56",
            "0020bdd6d2c142e79beb67e36177276c",
            "e0188e7486ea4281bef7c622de3f6ec2",
            "24398446e7954658bd5c65e8a2db55d6",
            "17ca7a85c36143129f96680a3ae5e967",
            "b2fd9aff3c8c4dd3b322d600ed14ace4",
            "8f9e178efd1e4304a6ce5f4ccd879d9a",
            "fe0ad95ce7d34ba0893defd0b2221691",
            "3c65b6d6520c4b6598be0f487a5d2292",
            "e597cfebb1ec4ab3b020b4afe3692d67",
            "643613c3a70747c4a025bc41a3465e75",
            "3b83c4ae59ff464e96b8e6cfc5b27d95",
            "07d54d920e21430a83841b424b0437b5",
            "7e2df541c3a0461aa707c02ac67ae659",
            "80a39cdb6e6d4d45808a5d9c3d0d0ee4",
            "983044128acc400e8216346db3b2a29e",
            "537de3934e994fe8b22532a1bf13f523",
            "9d8d7e6987b1471da3aeeb292bfe122e",
            "b25f148832f746cf8a964933ca66b910",
            "cc72bd2b840f4689b7fd6982fb80d957",
            "c1c5afe37961476fa9bc830d96ccdb3a",
            "2acbe5f82f6142f4a442fe139eea9221",
            "bf4388cacb8a4bd28e969d1e58ba979e",
            "29bbb0f5d2bc4ef99ee81dadbf5f579e",
            "481ba278b6834cfe8b6d8a48e6aaa23a",
            "c72c215a264b4ddbb2f339844f53f9ca",
            "ac7ac14ec8234d65b1c7098b88817d6b",
            "7e233595fbc04dacb5e1663738cf1a0b",
            "c4eec53198bc4220b3a3eb552a0f878a",
            "47abe176119d4cacb3dc0ce0646da888",
            "8bdfe1bc0d2e46a6903175ff08ee18de",
            "a0fd89e91a634713820d22efb81a147b",
            "7c31b53c81fe40a9a5158e879ba06854",
            "d7e45e167c2e41ec88b23bb97a9570c0",
            "13d752dafb3144358de8131bfe534342",
            "00fb3dcfb7854987a9ddc8588f35a5e4",
            "f144ae5849c24d1293c060024b8103d1",
            "a5967ccc95994490bbce05d0bdedf94a",
            "eff6bedb69a44381b281c83a42208e54",
            "24177efca21e46a2bf3c68a3c99a45d7",
            "e2d33cac596a43679d588aa5099e9b2d",
            "89f2c9ea50cc4cfb8b27a22159b30454",
            "4d325a7ec67c4a67a1cbae0f923a64b3",
            "e24ae97c845e4e269df5279753374d7f",
            "90499060cedf4b13bc627e3f1d802746",
            "838f96b7bc684e028d77b9cf192e0bf7",
            "e1354d037e9b41a48de78bb6c572b5e4",
            "99b4b6a6c8fd44fca322b9b74789e0e1",
            "f114f4e62e47470a821d4810528b556c",
            "ddb3d8997eb049a5add29c8baed0880d",
            "1e09450524ec4436958fd6c8edc3e12e",
            "52d179ccaa0044e2be546173d84a4411",
            "272a822ff1a3483bb9d4b47e513c8f22",
            "1cf5343bc4414d2fb6022cef09679284",
            "0a4c565a06324826b4424acaa2ba2a61",
            "e3a350b87e0d48c6991a04e98ca550fe",
            "c67332a76a704d8f85a3d3e41d53ef2f",
            "29cadc3b1e1546bbbfba43caf6bec01e",
            "90b940ea47624fe4aaa0204f9eeeeb78",
            "45bcdb9ad2384268b5a51c0897fd0207",
            "a9a9a1a2865546439a723b335cd180df",
            "95b411f6c3c04a2f844e99be0e8685f8",
            "be3bdfdfc4184097aae79c1aceae4703",
            "513a1211fd974597aa27d32869f677d9",
            "67dbf848eb5c4ce9984594890b9ae38f",
            "b61d9937dd0c4447aee5d4c5fb0955af",
            "20acfe44755a4daeab0f7edc7342c49d",
            "040f8992b4ff42af8cd3726782f8106c",
            "309942b8ec314e41a2815741dd1a2c34"
          ]
        },
        "id": "ZiLrhdCNo3nQ",
        "outputId": "3b2769fc-df02-4236-8294-00448fe31412"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48d539e070c645ef93ba88b926b4f5a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 4.9948 | Val Loss: 4.8062\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "751626604a814aa5971397d2865c627f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Loss: 4.7229 | Val Loss: 4.6815\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "346fd6c7b39f4f4b8750205207da2e95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Loss: 4.5860 | Val Loss: 4.5980\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0de2d21a7b948e3b528720aeb3cc9eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Loss: 4.4683 | Val Loss: 4.5495\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24398446e7954658bd5c65e8a2db55d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Loss: 4.3590 | Val Loss: 4.4883\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80a39cdb6e6d4d45808a5d9c3d0d0ee4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 6] Train Loss: 4.2538 | Val Loss: 4.4421\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c72c215a264b4ddbb2f339844f53f9ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 7] Train Loss: 4.1545 | Val Loss: 4.4020\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f144ae5849c24d1293c060024b8103d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 8/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 8] Train Loss: 4.0531 | Val Loss: 4.3727\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99b4b6a6c8fd44fca322b9b74789e0e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 9/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 9] Train Loss: 3.9551 | Val Loss: 4.3932\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90b940ea47624fe4aaa0204f9eeeeb78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 10/20:   0%|          | 0/1677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-e2b2e78a1b90>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  train_loss = 0\n",
        "  loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "  tr.train()\n",
        "  for feature in loop:\n",
        "    src = feature['input_ids'].to(config.device)\n",
        "    start = feature['start_positions'].to(config.device)\n",
        "    end = feature['end_positions'].to(config.device)\n",
        "    padding_mask = feature['attention_mask'].to(config.device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    start_output,end_output = tr(src,padding_mask)\n",
        "\n",
        "    start_loss = loss_fn(start_output,start)\n",
        "    end_loss = loss_fn(end_output,end)\n",
        "\n",
        "    total_loss = (start_loss + end_loss)/2\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(tr.parameters(), max_norm=1.0)\n",
        "\n",
        "    opt.step()\n",
        "    train_loss += total_loss.item()\n",
        "    loop.set_postfix(loss=total_loss.item())\n",
        "  avg_train_loss = train_loss / len(loader)\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "  tr.eval()\n",
        "  val_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for feature in val_loader:\n",
        "      src = feature['input_ids'].to(config.device)\n",
        "      start = feature['start_positions'].to(config.device)\n",
        "      end = feature['end_positions'].to(config.device)\n",
        "      padding_mask = feature['attention_mask'].to(config.device)\n",
        "      start_output,end_output = tr(src,padding_mask)\n",
        "      start_loss = loss_fn(start_output,start)\n",
        "      end_loss = loss_fn(end_output,end)\n",
        "\n",
        "      total_loss = (start_loss + end_loss)/2\n",
        "      val_loss += total_loss.item()\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_loader)\n",
        "  val_losses.append(avg_val_loss)\n",
        "\n",
        "  tqdm.write(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JCM6mGVo5cT"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def val():\n",
        "  tr.eval()\n",
        "  val_losses = 0\n",
        "  val_output = []\n",
        "  true = []\n",
        "  for feature in val_loader:\n",
        "    src = feature['input_ids'].to(config.device)\n",
        "    start = feature['start_positions'].to(config.device)\n",
        "    end = feature['end_positions'].to(config.device)\n",
        "    padding_mask = feature['attention_mask'].to(config.device)\n",
        "    start_output,end_output = tr(src,padding_mask)\n",
        "    val_output.append((start_output,end_output))\n",
        "    true.append((start,end))\n",
        "    start_loss = loss_fn(start_output,start)\n",
        "    end_loss = loss_fn(end_output,end)\n",
        "\n",
        "    total_loss = (start_loss + end_loss)/2\n",
        "    val_losses += total_loss.item()\n",
        "\n",
        "  return (val_losses / len(val_loader),val_output,true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Nw8HY8o6jF"
      },
      "outputs": [],
      "source": [
        "loss_out,val_output,true=val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTdTH-RIMh20",
        "outputId": "5cd0f138-441e-482d-8534-95c7a1b68a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.3983801868226795"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nID2aL-QNDZJ"
      },
      "outputs": [],
      "source": [
        "i = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzSLWFHfMtWk",
        "outputId": "3c921d05-2d76-41fa-ef1a-c84df32c6a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 20, 116, 194, 196],\n",
            "        [146, 120, 152, 152],\n",
            "        [ 22,  71, 208, 210],\n",
            "        [189, 126, 237, 239],\n",
            "        [ 31,  40,  59,  60],\n",
            "        [ 68, 175, 246, 250],\n",
            "        [ 53,  16,  14,  16],\n",
            "        [ 39, 167,  91,  94],\n",
            "        [ 23,  28, 115, 117],\n",
            "        [ 31,  20,  34,  35],\n",
            "        [ 56, 110,  91,  94],\n",
            "        [ 33,  23, 160, 161],\n",
            "        [179, 202,  59,  59],\n",
            "        [176,  54, 116, 116],\n",
            "        [134,  84, 145, 146],\n",
            "        [ 14,  32,  17,  22],\n",
            "        [179, 202,  59,  59],\n",
            "        [ 23,  43, 126, 131],\n",
            "        [ 17,  35,  17,  18],\n",
            "        [180,  77, 120, 120],\n",
            "        [180,  87, 224, 224],\n",
            "        [ 68,  46,  87,  87],\n",
            "        [ 71,  80, 173, 174],\n",
            "        [ 29,  33,  32,  33],\n",
            "        [ 36,  72,  81,  81],\n",
            "        [ 22,  58, 192, 196],\n",
            "        [ 18,  22,  21,  22],\n",
            "        [ 21, 150, 191, 195],\n",
            "        [ 37,  26,  89,  89],\n",
            "        [ 61, 192, 191, 192],\n",
            "        [ 63,  63, 214, 217],\n",
            "        [ 58,  28,  68,  77]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  v,start_index=val_output[i][0].topk(dim=-1,k=1)\n",
        "  v,end_index=val_output[i][1].topk(dim=-1,k=1)\n",
        "  print(torch.cat([start_index,end_index,true[i][0].unsqueeze(-1),true[i][1].unsqueeze(-1)],dim = -1))\n",
        "  i+=1\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDUypb1yKnWh"
      },
      "outputs": [],
      "source": [
        "v,start_index=val_output[0][0].topk(dim=-1,k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ct2QS0pLPmg"
      },
      "outputs": [],
      "source": [
        "v,end_index=val_output[0][1].topk(dim=-1,k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIgZO4zio8J1"
      },
      "outputs": [],
      "source": [
        "for idx,batch_output in enumerate(val_output):\n",
        "\n",
        "  for i in range(config.batch_size):\n",
        "    print(i)\n",
        "    print(torch.argmax(batch_output[0][i],dim=-1),torch.argmax(batch_output[1][i],dim=-1))\n",
        "    print(true[idx][0][i],true[idx][1][i])\n",
        "    print()\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
